Large language models (LLMs) pretrained on large amounts of publicly avail- able data have achieved widespread success. However, LLMs have been shown to memorize their training data, leading many organizations to ban the use of LLMs lest they leak private data. We present a new “neural phishing attack’ that presents the most concrete vulnerability to the threat model of real-world LLMs. We design targeted data poisoning attacks on LLMs that amplify an attacker’s ability to extract unknown secrets. One area of security threats to machine learning are data poisoning attacks. Privacy leakage from machine learning comes in three main forms of membership inference, attribute inference and data extraction. The last vulnerability primarily comes as a result of models memorizing data in a manner that can be extracted.